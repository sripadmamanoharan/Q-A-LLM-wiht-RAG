# -*- coding: utf-8 -*-
"""RAG-Basic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xem2QvH129vVXfUyVeGswCNRlykb61fF

âœ… Quick Recap of What Youâ€™ve Done:
Loaded an interesting document (healthcare_ai.txt)

Split it into chunks for processing

Created embeddings using Hugging Face

Stored them in a vector store (FAISS)

Loaded a small LLM (like BLOOM)

Built a RAG pipeline

Successfully queried it using .invoke()

Absolutely, Sri! Here's a **clear and simple explanation** of your **LLM + RAG project** in Colab â€” what it does, how it works, and why itâ€™s powerful.

---

# ğŸ“š Project Title:

### **â€œQuestion Answering using LLM + RAG on Real Documentsâ€**

---

## âœ… Project Goal:

To **ask questions** about any text file (like news, reports, research, or articles) and get smart, relevant answers using an LLM â€” **not from memory, but from the real content you provided.**

---

## ğŸ’¡ Why Use RAG Instead of Just LLM?

| Traditional LLM           | RAG (Retrieval-Augmented Generation)       |
| ------------------------- | ------------------------------------------ |
| Answers from memory only  | Answers based on real documents you give   |
| Might hallucinate facts   | Reduces hallucination by grounding answers |
| Canâ€™t handle recent data  | You control the data it sees               |
| Static (pre-trained only) | Dynamic â€” fetches, reads, and replies      |

---

## ğŸ§  Main Concepts (Simple Words)

### ğŸ”¹ LLM (Large Language Model)

A smart model like GPT or BLOOM that can understand and generate human-like text.

### ğŸ”¹ Retrieval

You split your documents into small parts (chunks), convert them into vectors (numbers), and store them in a **vector database (FAISS)** so they can be searched quickly.

### ğŸ”¹ Augmented Generation

Before the LLM answers your question, it searches your document chunks and finds the **most relevant pieces**, and gives those to the LLM. This helps the LLM generate accurate, grounded answers.

---

## ğŸ—ï¸ Components of the Project (with Explanation)

| Step | What You Did           | Why Itâ€™s Important                                  |
| ---- | ---------------------- | --------------------------------------------------- |
| 1ï¸âƒ£  | Install libraries      | You need LangChain, Transformers, etc.              |
| 2ï¸âƒ£  | Load a `.txt` file     | This is your knowledge source                       |
| 3ï¸âƒ£  | Chunk the text         | Breaks text into smaller readable pieces            |
| 4ï¸âƒ£  | Embed each chunk       | Convert text to vectors for fast search             |
| 5ï¸âƒ£  | Save to FAISS          | FAISS is the vector database                        |
| 6ï¸âƒ£  | Load LLM (e.g. BLOOM)  | It will read the relevant chunks and answer         |
| 7ï¸âƒ£  | Use RetrievalQA        | Combines LLM + retriever for RAG                    |
| 8ï¸âƒ£  | Query with `.invoke()` | Ask a question and get an answer based on your file |

---

## ğŸ” Real Example Flow (Behind the Scenes)

You ask:

> "How does AI help in diagnostics?"

What happens:

1. Your question is converted into a vector
2. FAISS searches your document and finds the most relevant 2â€“3 chunks
3. These chunks are passed to the LLM
4. The LLM reads those chunks and generates an accurate answer

---

## âœ… Benefits of This Project

* Understands **how RAG architecture works**
* Shows how to combine **vector search + LLM**
* You can expand it to work with PDFs, websites, etc.
* Real-world use in **healthcare, finance, legal, education**, and more

---

## ğŸ› ï¸ What You Can Do Next

| Add Feature   | What It Does                               |
| ------------- | ------------------------------------------ |
| Use PDFs      | Load research papers or reports            |
| Gradio UI     | Add a chat interface                       |
| Streamlit     | Build a dashboard-like app                 |
| Use Ollama    | Run it offline with LLaMA-3 locally        |
| Use your data | Apply RAG to customer feedback, news, etc. |

---

ğŸ“„ Your Text File (e.g., healthcare_ai.txt)
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 1. Text Splitting          â”‚
     â”‚  â†’ Break into chunks       â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 2. Embedding               â”‚
     â”‚  â†’ Convert chunks to vectors using HF model â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 3. Vector Store (FAISS)    â”‚
     â”‚  â†’ Save vectorized chunks  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

 You Ask a Question (e.g., "How does AI help in diagnostics?")
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 4. Query Embedding         â”‚
     â”‚  â†’ Convert your question to a vector â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 5. Retrieval               â”‚
     â”‚  â†’ Search FAISS for top relevant chunks â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 6. Generation (LLM)        â”‚
     â”‚  â†’ BLOOM or LLaMA generates answer â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
           âœ… Final Answer!
  "AI helps by detecting tumors in X-rays using vision models."
"""

!pip install langchain faiss-cpu transformers huggingface_hub

with open("sample.txt", "w") as f:
    f.write("""
Artificial Intelligence (AI) is transforming the healthcare industry by enabling faster diagnostics, personalized treatments, and predictive care.
AI algorithms can now analyze medical images, predict patient risks, and assist in robotic surgeries.

One major benefit is AI-powered diagnostics. Tools like computer vision can identify tumors or fractures in X-rays and MRIs with high accuracy.

Another important area is predictive analytics. By analyzing historical patient data, AI can forecast potential diseases and suggest early interventions.

Natural Language Processing (NLP) allows AI to process unstructured clinical notes and recommend treatments based on patient history.

However, ethical concerns like data privacy, bias, and explainability must be addressed to fully trust AI in healthcare.
Collaboration between doctors, data scientists, and policymakers is essential for safe and effective AI adoption.

The future of AI in healthcare is promising, aiming to improve outcomes while reducing costs and human error.
""")

!pip install -U langchain langchain-community faiss-cpu transformers huggingface_hub

from langchain.document_loaders import TextLoader

loader = TextLoader("sample.txt")
documents = loader.load()
print(documents[0].page_content)

from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)
docs = splitter.split_documents(documents)

print("Total Chunks:", len(docs))
print(docs[0].page_content)

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embedding_model = HuggingFaceEmbeddings()
vector_store = FAISS.from_documents(docs, embedding_model)

from transformers import pipeline
from langchain.llms import HuggingFacePipeline

pipe = pipeline("text-generation", model="bigscience/bloom-560m", max_new_tokens=100)
llm = HuggingFacePipeline(pipeline=pipe)

from langchain.chains import RetrievalQA

rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_store.as_retriever()
)

query = "What is LangChain?"
response = rag_chain.invoke(query)
print("Answer:", response)

rag_chain.invoke("What is AI doing in diagnostics?")
rag_chain.invoke("How can AI help reduce costs in healthcare?")
rag_chain.invoke("What ethical concerns are there with AI in medicine?")